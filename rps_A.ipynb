{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7f14e9-4312-4909-b820-5ef9978d5b4d",
   "metadata": {},
   "source": [
    "# Learning CFR: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208e23d-d174-404f-b0d3-55c83e27ed3b",
   "metadata": {},
   "source": [
    "### Regret matching: \n",
    "\n",
    "in a game of Rock, Paper, Scissor, you play rock and your opponent plays paper and you lose. You regret not having played paper or scissor. You want to use this to inform your next round of play. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a3d7b-d493-465f-99e8-c1a83394bd3d",
   "metadata": {},
   "source": [
    "#### Definitions:\n",
    "\n",
    "Action ($S_i$): the choices available for players eg R, P, S\n",
    "\n",
    "Each possible combination of simultaneous actions -> action profile (eg: I play Rock and he plays Paper, 'R,P' is now an action profile. $A = S_1 x S_2 x ... x S_n$\n",
    "\n",
    "Utility: reward and punishment of an action provided by a function $u_i()$ eg if we play for 1 dollar then utilities are +1, 0, -1\n",
    "\n",
    "Strategy: the probability of choosing a single action $\\sigma(s)$\n",
    "\n",
    "Expected utility (for a player): player probability X utility summed for each action profile for both players\n",
    "\n",
    "$u_i (\\sigma_i, \\sigma_-i) = \\sum \\sum \\sigma_i(s)  \\sigma_-i(s') u(s,s') $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1b4e1-0828-4ef0-baf7-5dc64dd42e03",
   "metadata": {},
   "source": [
    "#### The Regret matching algorithm:\n",
    "\n",
    "Regret (for an action we have not chosen): utility of action we didn't choose - utility of action we did choose\n",
    "\n",
    "eg Regret of not playing paper =  $u(paper, paper) - u(rock, paper)$\n",
    "\n",
    "How do we inform future plays based on that? \n",
    "\n",
    "We choose the action that we regret most not choosing. But, to avoid being predectbile, we select actions at random from a distribution proportional to positive regrets. \n",
    "\n",
    "Normalized regrets: to get proabulities -> positive regrets / their sum \n",
    "\n",
    "eg in RPS example: R,P,S -> $0, 1, 2 -> 0/3, 1/3, 2/3 -> (0, 1/3, 2/3)$\n",
    "\n",
    "We add previous regrets each step so we get cumulative regrets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfad97f-6ca7-4855-af62-f01491f03d82",
   "metadata": {},
   "source": [
    "#### The algorithm is as follows: \n",
    "\n",
    "```\n",
    "For each player, initialize all cumulative regrets to 0. \n",
    "\n",
    "For some number of iterations: \n",
    "- Compute a regret-matching strategy profile. (If all regrets for a player are non-positive, use a uniform random strategy.) \n",
    "- Add the strategy profile to the strategy profile sum. \n",
    "- Select each player action profile according the strategy profile. \n",
    "- Compute player regrets. \n",
    "- Add player regrets to player cumulative regrets.\n",
    "- Return the average strategy profile, i.e. the strategy profile sum divided by the number of iterations.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb80d6-21b8-4bf5-8c84-173edfc85fb5",
   "metadata": {},
   "source": [
    "#### Concepts inventory:\n",
    "\n",
    "Strategy profile: we need a probability distribution for each player (a function that computes it). \n",
    "\n",
    "Action: a function that selects next action at random from the probability distribution (strategy profile). \n",
    "\n",
    "Utility: we need a function to compute utilities for each player based on their selected actions eg $u(R, S)$\n",
    "\n",
    "Regrets: we need to compute regrets based on utilities (see above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f41f106-7c2c-4476-837a-9c0c82c47fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54d351fc-9760-4f4a-8da8-9b966a945b20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(['R', 'P', 'S'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "07dd903f-2137-4812-aa85-d5eed78cdee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one player - opponent - will be static [0.3, 0.3, 0.4] -> this is his Strategy Profile\n",
    "# we need to compute it for the other player\n",
    "# R = 0, P = 1, S = 2 -> we represent them as indices\n",
    "\n",
    "actions = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3f87863b-724c-48f5-9f00-9bf72cb7fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRegrets(a):\n",
    "    '''\n",
    "    Takes as input a -> action_profile [0,0] returns [0, 1, -1] for (R, P, S).\n",
    "    We do calculation for one player since opponent is fixed.\n",
    "    '''\n",
    "    regrets = [0, 0, 0]\n",
    "    acts = [0, 1, 2]\n",
    "    player_utilities = np.array([[0, -1, 1], [1, 0, -1], [-1, 1, 0]])\n",
    "    \n",
    "    regrets[a[0]] = 0 # we have no regrets for the action we choose\n",
    "    acts.remove(a[0])    \n",
    "    \n",
    "    for i in actions:\n",
    "        # regret = utility(not chosen) - utility(chosen)\n",
    "        regrets[i] = player_utilities[i, a[1]] - player_utilities[a[0], a[1]]\n",
    "    return regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "339034c7-7f79-4798-9593-b029a69501f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = getRegrets([0,1])\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b9693f8d-7a44-4e9e-bed9-88838f3d651d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.33333333, 0.66666667])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r / sum(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "474b45e1-1434-4ea6-9418-1bf42497fb36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92054ebb-a308-4daf-a101-3eb4bc6f6f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_utilities = np.array([[0, -1, 1], [1, 0, -1], [-1, 1, 0]])\n",
    "a = [0, 1]\n",
    "player_utilities[a[0], a[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3a83caa0-01cb-4d8b-9f67-98dbb2ac1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStrategy(regrets):\n",
    "    '''\n",
    "    Return a regret-matching strategy profile of the form [0.3, 0.4, 0.4]\n",
    "    If all regrets for a player are non-positive, use a uniform random strategy.\n",
    "    '''\n",
    "    if sum(regrets) <= 0: return 1.0 / np.array([3, 3, 3])\n",
    "    else: \n",
    "        normalized_regrets = regrets / sum(regrets)\n",
    "        \n",
    "    return normalized_regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "233dadfe-0656-4802-b5ff-f2458792dd72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_action_profile(playerSt, opponentSt):\n",
    "    player = np.random.choice(a = actions, p = playerSt)\n",
    "    opponent = np.random.choice(a = actions, p = opponentSt)\n",
    "    return [player, opponent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "39b0a2d3-f847-4d1f-a57e-55ba8a1d5a93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_action_profile(np.array([0.3, 0.3, 0.4]), np.array([0.3, 0.3, 0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d4944597-495b-46c3-883f-b42768a7bccf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerRegrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e054aac7-3dfb-4773-bea9-d2f92e166f73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41566667 0.251      0.33333333]\n"
     ]
    }
   ],
   "source": [
    "#opponentStrategy = np.array([0.3, 0.3, 0.4])\n",
    "\n",
    "opponentStrategy = np.array([0.0, 0.0, 1.0])\n",
    "\n",
    "playerStrategy = np.zeros(3)\n",
    "playerRegrets = np.zeros(3)\n",
    "cumulativeRegrets = np.zeros(3)\n",
    "strategySum = np.zeros(3)\n",
    "\n",
    "iters = 1000\n",
    "\n",
    "for i in range(iters):\n",
    "    playerStrategy = getStrategy(playerRegrets)\n",
    "    strategySum += playerStrategy\n",
    "    action_profile = select_action_profile(playerStrategy, opponentStrategy)\n",
    "    playerRegrets = getRegrets(action_profile)\n",
    "    cumulativeRegrets += playerRegrets\n",
    "\n",
    "print(strategySum / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "05297ddd-9088-4880-ab3c-2a1735d2f7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999997"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((strategySum / iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff3696-15ab-4000-8aac-1fe4f90e50a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
